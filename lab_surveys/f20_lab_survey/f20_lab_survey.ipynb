{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey = pd.read_csv('f20_lab_survey.csv')\n",
    "survey = survey[[c for c in list(survey.columns) if c[0:2] == '41']]\n",
    "survey = survey[[c for c in survey.columns if c.split(':')[0] not in ['410964', '410961', '410968']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_rename = {}\n",
    "index_to_question = {}\n",
    "question_to_index = {}\n",
    "\n",
    "cols = list(survey.columns)\n",
    "for i in range(len(cols)):\n",
    "    col = cols[i]\n",
    "    q = col.split(': ')[1]\n",
    "    column_rename[col] = i+1\n",
    "    index_to_question[i+1] = q\n",
    "    question_to_index[q] = i+1\n",
    "\n",
    "survey = survey.rename(columns=column_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response scales\n",
    "AGREE_SCALE = {1 : 'Strongly Disagree',\n",
    "               2 : 'Disagree',\n",
    "               3 : 'Neutral',\n",
    "               4 : 'Agree',\n",
    "               5 : 'Strongly Agree'}\n",
    "HARDNESS_SCALE = {1 : 'Too Hard',\n",
    "                  3 : 'Just Right',\n",
    "                  5 : 'Too Easy'}\n",
    "DIFFICULTY_SCALE = {1 : 'Easy',\n",
    "                    5 : 'Hard'}\n",
    "CLARITY_SCALE = {1 : 'Poor',\n",
    "                 5 : 'Clear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set scale for each question\n",
    "scales = {}\n",
    "for col in survey.columns:\n",
    "    if int(col) in [1,2,3,5,11,12,13,14,17,18,20,22,23,24,25,26,27]:\n",
    "        scales[col] = AGREE_SCALE\n",
    "    elif int(col) == 4:\n",
    "        scales[col] = HARDNESS_SCALE\n",
    "    elif int(col) == 15:\n",
    "        scales[col] = DIFFICULTY_SCALE\n",
    "    elif int(col) == 16:\n",
    "        scales[col] = CLARITY_SCALE\n",
    "    else:\n",
    "        scales[col] = None\n",
    "survey = survey.append(scales, ignore_index=True).rename(index={77:'scale'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey = survey.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agree_questions = survey[survey.scale == AGREE_SCALE].drop(columns='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_questions(scale_constant):\n",
    "    questions = survey[survey.scale == scale_constant].drop(columns='scale')\n",
    "    statistics = {}\n",
    "    \n",
    "    for index, row in questions.iterrows():\n",
    "        stats = {}\n",
    "        stats['NA'] = sum(row.isna())\n",
    "        for i in range(1,6):\n",
    "            stats[i] = 0\n",
    "        series = pd.Series([int(i.split(' - ')[0]) for i in row[~row.isna()]])\n",
    "        stats.update(series.value_counts().to_dict())\n",
    "        stats['avg'] = round(series.mean(),2)\n",
    "        statistics[index] = stats\n",
    "        \n",
    "    if len(statistics) == 1:\n",
    "        i = list(statistics)[0]\n",
    "        row = list(statistics.values())[0]\n",
    "        statistics = pd.DataFrame(row, index=[i])[['NA',1,2,3,4,5,'avg']].reset_index()\n",
    "    else:\n",
    "        statistics = pd.DataFrame(statistics).transpose()[['NA',1,2,3,4,5,'avg']].reset_index()\n",
    "    statistics['index'] = statistics['index'].apply(lambda x: index_to_question[x])\n",
    "    statistics = statistics.rename(columns={'index' : 'question'})\n",
    "\n",
    "    scale = {c : None for c in statistics.columns}\n",
    "    scale.update(scale_constant)\n",
    "    return pd.concat([pd.DataFrame(scale, index=[0]), statistics], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_ranks(ls):\n",
    "    ls = [i.split(',') for i in ls[~ls.isna()]]\n",
    "    ls = pd.Series([item for sublist in ls for item in sublist])\n",
    "    return pd.DataFrame(ls.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranks for top 3 labs and bottom 3 labs\n",
    "top_labs = lab_ranks(survey.loc[6])\n",
    "bottom_labs = lab_ranks(survey.loc[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter vs. Web-based question\n",
    "pref = pd.DataFrame(survey.loc[19].value_counts()).transpose()\n",
    "pref['question'] = index_to_question[19]\n",
    "pref = pref[['question','Web-based', 'Jupyter Notebook']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(i):\n",
    "    ls = survey.loc[i]\n",
    "    ls = list(ls[~ls.isna()])\n",
    "    return '\\n\\n'.join(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = open(\"f20_report.md\", \"w\")  \n",
    "report.write('# **Fall 2020 Lab Survey** \\n\\n')\n",
    "report.write('\\n\\n ## **Numerical Questions** \\n\\n')\n",
    "report.write(scale_questions(AGREE_SCALE).to_html(buf=None))\n",
    "report.write(scale_questions(HARDNESS_SCALE).to_html(buf=None))\n",
    "report.write(scale_questions(DIFFICULTY_SCALE).to_html(buf=None))\n",
    "report.write(scale_questions(CLARITY_SCALE).to_html(buf=None))\n",
    "report.write(pref.to_html(buf=None))\n",
    "report.write('\\n\\n ## **Appearances under \"Top 3 Labs\"** \\n\\n')\n",
    "report.write(top_labs.to_html(buf=None))\n",
    "report.write('\\n\\n ## **Appearances under \"Bottom 3 Labs\"** \\n\\n')\n",
    "report.write(bottom_labs.to_html(buf=None))\n",
    "\n",
    "report.write('\\n\\n ## **Text Questions** \\n\\n')\n",
    "for i in [7,8,10,21,28,29,30,31]:\n",
    "    report.write('\\n\\n ### **%s** \\n\\n' % (index_to_question[i]))\n",
    "    report.write(process_text(i))\n",
    "        \n",
    "report.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!grip f20_report.md"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
